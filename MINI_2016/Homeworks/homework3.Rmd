---
title: "Homework3"
author: "Klaudia Magda"
date: "11 listopada 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



#Libraries
```{r, message=FALSE, warning=FALSE}
library(e1071)
library(caret)
library(magrittr)
library(plyr)
library(dplyr)
library(corrplot)
library(randomForest)
library(pROC)
library(Epi)
library(nnet)
library(party)
library(rpart)

```

## Load dataStudent


```{r, echo=TRUE, message=FALSE, warning=FALSE, fig.align='center'}
data<-read.csv(file="student-mat.csv", sep=";")


```


##Preprocessing
Preprocessing will help us to reduce amount of predictors. Chosen methods: Removing near zero variance variables (if exists), normalisation and removing highly correlated variables
```{r, echo=TRUE, message=FALSE, warning=FALSE}


#Removing near zero variance variables
nzv <- nearZeroVar(data)
names(data)[nzv]
#Normalisation
preProcValues <- preProcess(data, method = c("range"))
dataNorm <- predict(preProcValues, data)
#Removing highly correlated variables
d.num <- dataNorm %>% select(which(sapply(dataNorm, is.numeric)))
too_high <- findCorrelation(cor(d.num), cutoff = 0.725, verbose = FALSE)
names(d.num)[too_high]
data = dataNorm[,-c(too_high)]

```

#Feature selection
After data preparation there is a need to choose only the best variables for classification.  decided to use a `filter` method - Selection by filter (`sbf`) on a smaller sample of the dataset to receive higher accuracy (for bigger dataset accuracy is very low).

We can notice that the best variables to classifiers are correlated G1 and G2. 
```{r, echo=TRUE, message=FALSE, warning=FALSE}
size <- floor(0.05 * nrow(data))
indxSample <- sample(seq_len(nrow(data)), size = size)
dataSmall <- data[indxSample, ]
length(levels(dataSmall$Class))


length(levels(dataSmall$G3))
#SBF
filterCtrl <- sbfControl(functions = rfSBF, method = "repeatedcv", verbose = FALSE, repeats = 5)
rfWithFilter <- sbf(form = G3 ~ ., data = dataSmall, sbfControl = filterCtrl, allowParallel = TRUE, variables=TRUE)
rfWithFilter

```
#Division
Division is made 75:25
```{r, echo=TRUE, message=FALSE, warning=FALSE}
set.seed(1313)
size <- floor(0.75 * nrow(dataSmall))
indxTrain <- sample(seq_len(nrow(dataSmall)), size = size)

dataSmallTrain <- dataSmall[indxTrain, ]
dataSmallTest <- dataSmall[-indxTrain, ]
```
#Forest
Forest is a set of multiple amount of trees. Here -> for every variable
```{r, echo=TRUE, message=FALSE, warning=FALSE, fig.align='center'}
forest <- randomForest(G3 ~ ., data = dataSmallTrain, importance = TRUE, na.action = na.omit)
varImpPlot(forest)
importance(forest)
predForest = predict(forest, dataSmallTest, type="class")
plot(forest, main="Model Fit for Random Forest")
forestTab <- table(true = dataSmallTest$G3, predicted = predForest)
rfAcc <- sum(diag(forestTab)) / sum(forestTab)
rfAcc
```

#Top Variables
According to results of filtering, it is sufficient to make classification based on 2 top variables. 
G1 G2


Those variables give 100%.
```{r, echo=TRUE, message=FALSE, warning=FALSE, fig.align='center'}
forestTop <- randomForest(G3 ~ G1 + G2, data = dataSmallTrain, importance = TRUE, na.action = na.omit)
varImpPlot(forestTop)
importance(forestTop)
predForestTop = predict(forestTop, dataSmallTest, type="class")
plot(forestTop, main="Model Fit for Random Forest")
forestTopTab <- table(true = dataSmallTest$G3, predicted = predForestTop)
rfTopAcc <- sum(diag(forestTopTab)) / sum(forestTopTab)
rfTopAcc
```


##Tree
Tree is undirected graph with nodes and leaf nodes that can indicate the value of Class.
```{r, echo=TRUE, message=FALSE, warning=FALSE}

tree<-ctree(G3 ~ ., data=dataSmallTrain)
predTree = predict(tree, dataSmallTest)
plot(tree, main="Model Fit for Tree")
treeTab <- table(true = dataSmallTest$G3, predicted = predTree)
treeAcc <- sum(diag(treeTab)) / sum(treeTab)
treeAcc
```

##Tree
Tree for TopVariables

```{r, echo=TRUE, message=FALSE, warning=FALSE}

treeTop<-ctree(G3 ~ G1 + G2, data=dataSmallTrain)
predTreeTop = predict(treeTop, dataSmallTest)
plot(treeTop, main="Model Fit for Tree")
treeTabTop <- table(true = dataSmallTest$G3, predicted = predTreeTop)
treeTopAcc <- sum(diag(treeTabTop)) / sum(treeTabTop)
treeTopAcc


```


#Conclusions

To sum up, we can notice that random forest has a much better accuracy than tree-classifier (twice better). Also there is no affect on performance if there are just considered 2 variables instead of every variables. I consider that random forest is most effective, because consists a lot of trees and then computing is more accurate.

Moreover, value of accuracy has increased in such case that we divided dataset in smaller part. It is based on statement that classification works better for smaller amount of observations.
```{r, echo=FALSE, message=FALSE, warning=FALSE}
sum_table <- matrix(c(rfAcc,rfTopAcc, treeAcc, treeTopAcc), ncol=1, nrow = 4 , byrow = TRUE)
colnames(sum_table) <- c("Accuracy")
rownames(sum_table) <- c("RF", "RFTOP", "TREE", "TREETOP")
sum_table
```