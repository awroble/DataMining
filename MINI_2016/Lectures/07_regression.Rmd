---
title: "Regression"
author: "Anna Wróblewska"
date: "Data Mining - Advances"
output: 
  html_document:
    toc: TRUE
---

# Regression analysis

Regression analysis is used to describe the relationship between:

* A single response variable: $Y$ and
* One or more predictor variables: $X_1$, $X_2$, . . . , $X_p$
    - when $p = 1$ is Simple Regression
    - when $p > 1$ is Multivariate Regression

The response variable $Y$ must be a continuous variable.
The predictors $X_1$, $X_2$, . . . , $X_p$ can be continuous, discrete or categorical variables.
Linear regression objective is to generalize the simple regression methodology in order to describe the relationship between a response variable $Y$ and a set of predictors $X_1$, $X_2$, . . . , $X_p$ called exploratory variables.

So we have sets of observations: $(x_{11},...,x_{1p},y_1),...,(x_{n1},...,x_{np},y_n)$

Multivariate linear regression model is given by:
$y_i =\beta_0 +\beta_1*x_{i1} +\beta_2*x_{i2} +...+\beta_p*x_{ip} +ε_i$ for $i =1,...,n$ where:

* random error is $ε_i ∼ N(0, σ^2)$
* independent linear function is $\beta_1*x_{1} +\beta_2*x_{2}+ ... +\beta_p*x_{p}=E(Y|x_1,...x_p)$

Unknown parameters:

* $\beta_0$ is overall mean
* $\beta_k$, $k = 1,...,p$ are regression coefficients

##Estimation of regression line

As in the case of simple linear regression, we want to find the equation of the line that “best” fits the data. In this case, it means finding $b_0, b_1, ... , b_p$ such that the fitted values of $y_i$, given by

$\hat{y}_i =b_0+b_1*x_{1i} +...+ b_p*x_{pi}$, 

are as “close” as possible to the observed values $y_i$.

The difference between the observed value $y_i$ and the fitted value $\hat{y}_i$ is called *residual* and is given by:

$e_i = y_i − \hat{y}_i$

A way of calculating $b_0, b_1, ... , b_p$ is based on the minimization of the sum of the squared residuals, or residual sum of squares $RSS$ (Least Squares Method):

$RSS  =  \sum_i{e_i^2} =  \sum_i{(y_i−\hat{y}_i)^2} = \sum_i{(y_i −b_0 −b_1*x_{1i} −...− b_p*x_{pi})^2}$

The parameters $b_0, b_1, ... , b_p$ are estimated by using the function `lm()`.

#Some tips on exploratory data analysis                       
Once again we use pima dataset on diabetes in Pima Indian women.
```{r}
library(faraway)
pima$test <- factor(pima$test)
levels(pima$test) <- c("neg", "pos")
summary(pima)
hist(pima$glucose)
```
Note that categorical data should not be numerical, e.g. "test" values.

Variables “glucose”, “diastolic”, “triceps”, “insulin” and “bmi” have minimum value equal to zero. Is it possible to have glucose at zero value level? It seems that zero was used to code missing data. It should not be done because it can cause misleading results because for some variables it can be a valid value.

Set the missing values coded as zero to NA.
```{r}
pima$glucose[pima$glucose==0] <- NA
pima$diastolic[pima$diastolic==0] <- NA
pima$triceps[pima$triceps==0] <- NA
pima$insulin[pima$insulin==0] <- NA
pima$bmi[pima$bmi==0] <- NA

summary(pima) #summary with number of NA's
```

At first it is worth to do some graphical exploratory analysis one and more variables.
The scatter plot allows one to obtain an overview of the relations between variables.
```{r}
hist(pima$glucose)
plot(density(pima$glucose,na.rm=TRUE))
plot(triceps~bmi, pima)
boxplot(diabetes~test ,pima)
plot(pima, gap=0)
```
Now try a regression. Form the `pairs` plot we can see that there some correlated variables.
Fit the regression model using the function `lm()` and use a function `summary()` to get some results.
```{r}
pima.lm <- lm(glucose~insulin, data= pima)
summary(pima.lm, corr=TRUE)
```
`Intercept` is a free coefficient. 

#Regression excersices
Now we will use an excersice from ISRL book. 
```{r}
library(MASS)
data(Boston)
head(Boston)
?Boston
```
Please do some exploratory analysis of this dataset: summaries, plots, e.g. the range of `medv` column and its quantiles.
```{r}
lm.fit=lm(medv~lstat ,data=Boston)  
#Short information
lm.fit
summary(lm.fit)
```

# Class excercise and the homework

